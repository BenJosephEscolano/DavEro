import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA

# --- CONFIGURATION ---
st.set_page_config(page_title="Voice Style Analysis", layout="wide")
DATA_PATH = "final_project_data/multimodal_features.csv"


# --- LOADER ---
@st.cache_data
def load_data():
    try:
        df = pd.read_csv(DATA_PATH)
        return df
    except FileNotFoundError:
        return None


df_raw = load_data()

# --- ERROR HANDLING ---
if df_raw is None:
    st.error("üö® Data not found! Please run 'prepare_data.py' first.")
    st.stop()

# --- SIDEBAR ---
with st.sidebar:
    st.title("Final Project")
    st.caption("CS365-F2 | Group: Baithon") # This creates the faded look

page = st.sidebar.radio("Navigate", [
    "1. Project Overview",
    "2. Data Exploration & Methodology",
    "3. Analysis & Insights",
    "4. Conclusions"
])

# --- GLOBAL DATA PREP (Normalize Once) ---
# We normalize features 0-1 for all analysis to keep math consistent
features = ['loudness', 'breathiness', 'pitch', 'brightness', 'tonality']
scaler = MinMaxScaler()
df_norm = df_raw.copy()
df_norm[features] = scaler.fit_transform(df_raw[features])

# --- PAGE 1: OVERVIEW ---
if page == "1. Project Overview":
    st.title("Engineering Intimacy: The Paralinguistics of Voice")
    st.markdown("### What it's about: Project Overview")
    # 1. THE HOOK (The Hybrid Approach)
    with st.expander("Abstract: Research Question & Hypothesis"):
        st.info("""
        **Research Question:** Can we mathematically "Engineer" the feeling of intimacy?
    
        **The Hypothesis:** Intimacy is not random. It is a specific **Paralinguistic Code** composed of breathiness, proximity, and rhythm. 
        By analyzing **2,000** clips of hyper-stylized voice acting, we aim to reverse-engineer this code to prove that "Emotion" is just a vector of numbers.
        """)

    # 2. THE TRANSITION (Why this dataset?)
    st.markdown("""
    **Why this dataset?** We selected the *NandemoGHS* dataset because it represents **"Performative Intimacy."** The voice actors are not just speaking; they are utilizing extreme paralinguistic control to simulate closeness. This makes it the perfect laboratory to study the **Proximity Effect**.
    """)

    st.markdown("### Why its relevant: A Digital Prosthetic for Loneliness")
    st.markdown("""
        > "Against the backdrop of a global **male loneliness epidemic**, the demand for 'synthetic intimacy'‚Äîfrom ASMR to AI companions‚Äîhas skyrocketed. 
        > This dataset represents the industrial response to that crisis: a highly optimized, paralinguistic simulation of closeness designed to soothe social isolation. 
        > By decoding the acoustic mechanics of this performance, we are not just analyzing voice acting; **we are reverse-engineering the modern digital prosthetic for human connection.**"
        """)

    st.divider()

    # 3. THE TOOLKIT (Linguistics meets Physics)
    st.markdown("### Acoustic Features")
    st.write("We deconstruct the voice into 5 acoustic dimensions, mapping physics to linguistic meaning:")

    # This table bridges your two interests perfectly
    data_dict = pd.DataFrame([
        {"Feature": "Breathiness (ZCR)", "Linguistic Concept": "Phonation Type",
         "Role": "The 'Whisper Factor'. Essential for simulating physical closeness."},
        {"Feature": "Loudness (RMS)", "Linguistic Concept": "Prosodic Stress",
         "Role": "Inverse of Intimacy. Low energy creates a 'Private Space'."},
        {"Feature": "Pitch (F0)", "Linguistic Concept": "Intonation",
         "Role": "Defines the archetype (e.g., High=Vulnerable, Low=Dominant)."},
        {"Feature": "Speed (Char/Sec)", "Linguistic Concept": "Tempo",
         "Role": "Information density. Slow tempo signals comfort/seduction."},
        {"Feature": "Tonality (Flatness)", "Linguistic Concept": "Vocal Stability",
         "Role": "Tremble/Jitter signals emotional instability (High Arousal)."}
    ])
    st.table(data_dict)

    st.divider()

    # 4. DATASET METRICS
    col1, col2, col3 = st.columns(3)
    col1.metric("Sample Size", len(df_raw), "Clips Analyzed")
    col2.metric("Avg Prosodic Pitch", f"{df_raw['pitch'].mean():.0f} Hz")
    col3.metric("Semantic Density", "Sparse",
                help="Most files contain functional dialogue, with rare spikes of high emotion.")

    # 5. RAW DATA INSPECTION
    with st.expander("üîç Inspect the Raw Signal"):
        st.dataframe(df_raw.head(50))

# ... inside app.py ...

elif page == "2. Data Exploration & Methodology":
    st.title("Data Exploration & Methodology")
    st.markdown("""
    To analyze "Intimacy," we needed to transform raw audio into structured data. 
    This section details our **ETL (Extract, Transform, Load) Pipeline** and explores the statistical distribution of the resulting features.
    """)

    # --- SECTION 1: METHODOLOGY PIPELINE ---
    st.subheader("1. The Engineering Pipeline")
    st.info("We utilized a 4-stage process to prepare the data for analysis:")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.markdown("#### 1. Ingestion")
        st.caption("Streamed from Hugging Face (`NandemoGHS`).")
        st.markdown("‚¨á**Shuffle Buffer:** 1k items to prevent bias.")
    with col2:
        st.markdown("#### 2. Pre-Processing")
        st.caption("Standardization for ASR.")
        st.markdown("**Resampling:** Downsampled to **16kHz Mono**.")
    with col3:
        st.markdown("#### 3. Extraction")
        st.caption("Spectral Analysis via `Librosa`.")
        st.markdown("**5 Features:** Pitch, RMS, ZCR, Centroid, Flatness.")
    with col4:
        st.markdown("#### 4. Cleaning")
        st.caption("Quality Control.")
        st.markdown("**Filter:** Removed silent clips ($F_0=0$) and null text.")

    st.divider()

    # --- SECTION 2: DATA INTEGRITY CHECK ---
    st.subheader("2. Data Integrity Check")

    c1, c2 = st.columns(2)
    with c1:
        st.write("**Missing Value Inspection:**")
        # Check for nulls (should be 0 if our script worked)
        nulls = df_raw.isnull().sum()
        if nulls.sum() == 0:
            st.success("‚úÖ No missing values detected.")
        else:
            st.warning("‚ö†Ô∏è Missing values found (handled in visualizations).")
            st.dataframe(nulls)

    with c2:
        st.write("**Outlier Removal (Silence):**")
        st.caption("We filtered out clips where the Fundamental Frequency ($F_0$) was 0Hz (Silence/Noise).")
        st.metric("Cleaned Sample Size", len(df_raw))

    st.divider()

    # --- SECTION 3: FEATURE DISTRIBUTIONS ---
    st.subheader("3. Feature Distributions (Histograms)")
    st.markdown("How is the 'Style' distributed across the dataset?")

    # Interactive Selector
    feature_to_plot = st.selectbox(
        "Select Feature to Visualize",
        ["semantic_score", "pitch", "loudness", "breathiness", "speed"],
        format_func=lambda x: x.replace("_", " ").title()
    )

    # Logic for specific comments based on selection
    comment = ""
    if feature_to_plot == "semantic_score":
        comment = "üëâ **Insight:** The distribution is **Right-Skewed** (Power Law). High intimacy is rare; most dialogue is functional."
    elif feature_to_plot == "pitch":
        comment = "üëâ **Insight:** The distribution is **Bimodal** (Two peaks), suggesting distinct character types (High-pitched 'Moe' vs. Low-pitched 'Mature')."
    elif feature_to_plot == "breathiness":
        comment = "üëâ **Insight:** High breathiness (>0.2) represents the 'ASMR/Whisper' zone."

    fig_hist = px.histogram(
        df_raw,
        x=feature_to_plot,
        nbins=30,
        title=f"Distribution of {feature_to_plot.replace('_', ' ').title()}",
        color_discrete_sequence=['#636EFA'],
        marginal="box"  # Adds a boxplot on top to show outliers
    )
    st.plotly_chart(fig_hist, use_container_width=True)
    st.info(comment)

    # --- SECTION 4: CORRELATION HEATMAP ---
    st.subheader("4. Correlation Heatmap")
    st.markdown("Do louder voices tend to have higher pitch? (Testing the Physics)")

    # Numeric columns only
    corr_cols = ['loudness', 'breathiness', 'pitch', 'brightness', 'tonality', 'speed', 'semantic_score']
    corr_matrix = df_raw[corr_cols].corr()

    fig_corr = px.imshow(
        corr_matrix,
        text_auto=".2f",
        color_continuous_scale="RdBu_r",  # Red = Positive, Blue = Negative
        aspect="auto",
        title="Feature Correlation Matrix"
    )
    st.plotly_chart(fig_corr, use_container_width=True)

    with st.expander("Show Interpretation of Heatmap"):
        st.markdown("""
        * **Breathiness & Brightness (+0.72):** The strongest link in the dataset. Whispering generates high-frequency "hiss," proving our texture analysis is accurate.
        * **Breathiness & Tonality (-0.47):** Strong negative correlation. As expected, "breathy" whispers lack tonal stability.
        * **Loudness & Pitch (-0.23):** Surprisingly weak negative correlation. This suggests variety: actors perform both "Low/Loud" (Dominant) and "High/Loud" (Panic) scenes.
        * **Semantic Score (~0.0):** Almost no correlation with acoustics. This proves that **what** is said is independent of **how** it is said‚Äîa sign of complex acting.
        """)
    st.divider()

    # --- SECTION 5: DETAILED METHODOLOGY (THE THEORY) ---
    st.subheader("5. Algorithm Methodology")
    st.markdown(
        "We employed a multi-modal approach, combining signal processing physics with linguistic rule-based analysis.")

    # A. Acoustic Extraction
    with st.expander("Acoustic Feature Extraction (Librosa)"):
        st.markdown("""
            **Objective:** To quantify the "Paralinguistic Signal."

            We treated the audio as a time-series signal and extracted features using the **Librosa** library. 
            Instead of using raw waveforms (which are high-dimensional and noisy), we extracted 5 spectral proxies:

            1.  **Energy (RMS):** Calculated as the root mean square of the waveform amplitude. Used as a proxy for **Arousal**.
            2.  **Texture (Zero-Crossing Rate):** The rate at which the signal changes sign. High rates indicate "noisy" signals (fricatives/whispers), used as a proxy for **Intimacy/Proximity**.
            3.  **Pitch ($F_0$):** Extracted using the **YIN Algorithm**. We filtered values below 60Hz and above 1000Hz to isolate human vocal range.
            4.  **Timbre (Spectral Centroid):** The "center of mass" of the spectrum. Used to distinguish "Bright/Anime" voices from "Dark/Mature" voices.
            5.  **Stability (Spectral Flatness):** Measures how noise-like a sound is. Low flatness indicates a pure tone; high flatness indicates breath or trembling.
            """)

    # B. Semantic Analysis
    with st.expander("Semantic Analysis (Rule-Based NLP)"):
        st.markdown("""
            **Objective:** To quantify the "Semantic Symbol" (Intent).

            Standard NLP models (BERT/Transformers) fail on this dataset due to the high frequency of non-grammatical utterances (stuttering, moaning, slang).

            **Our Solution: Domain-Specific Lexicon Analysis**
            We engineered a weighted dictionary of genre-specific triggers (e.g., *Suki, Aishiteru, Motto*). 
            To prevent double-counting (e.g., finding "Love" inside "Cute"), we implemented a **Recursive Longest-Match Consumption Algorithm**:
            1.  Sort lexicon phrases by length (Longest ‚Üí Shortest).
            2.  Scan text for matches.
            3.  Score and **remove** the matched phrase from the string.
            4.  Repeat until the string is empty.
            """)

    # C. Unsupervised Learning
    with st.expander("Unsupervised Learning (K-Means & PCA)"):
        st.markdown("""
            **Objective:** To discover "Style Tokens" without human bias.

            We hypothesize that emotional archetypes are mathematically distinct clusters in high-dimensional feature space.
            * **Algorithm:** K-Means Clustering ($K=5$).
            * **Normalization:** Min-Max Scaling (0-1) to ensure Pitch (Hz) doesn't overpower Loudness (dB).
            * **Visualization:** We use **PCA (Principal Component Analysis)** to reduce the 5 dimensions down to 2D for visualization, proving that the clusters are distinct "islands" of emotion.
            """)
    # D. Feature Engineering (Physics)
    with st.expander("Feature Engineering (The Physics of Intimacy)"):
        st.markdown("""
        **Objective:** To Operationalize the "Proximity Effect."

        We hypothesized that "Intimacy" and "Intensity" are orthogonal vectors, similar to the **Russell Circumplex Model of Affect**.
        We engineered two composite metrics based on audio physics:

        1.  **Calculated Intimacy (Proximity):** * *Formula:* `(Breathiness * 1.5) + ((1 - Loudness) * 1.0)`
            * *Logic:* Physical closeness creates the "Proximity Effect" (Bass boost + Airiness). Therefore, High Breathiness + Low Volume = High Intimacy.

        2.  **Calculated Intensity (Arousal):**
            * *Formula:* `(Loudness * 1.5) + (Pitch * 1.0)`
            * *Logic:* High physiological arousal results in increased sub-glottal pressure (Loudness) and vocal cord tension (Pitch).
        """)

    # E. Cross-Modal Validation
    with st.expander("Cross-Modal Validation (Signal vs. Symbol)"):
        st.markdown("""
        **Objective:** To measure Acting Consistency (Congruence).

        We treat the dataset as a dual-stream information source:
        1.  **The Symbol (Text):** What the script says (Semantic Score).
        2.  **The Signal (Audio):** How the actor performs it (Acoustic Score).

        By plotting these two normalized vectors against each other, we can identify:
        * **Congruence:** Points on the diagonal (Good Acting).
        * **Dissonance:** Points far from the diagonal (Irony, Subversion, or Bad Acting).
        """)

# ... inside Page 2 logic ...
# ... inside app.py ...

elif page == "3. Analysis & Insights":
    st.title("üß† Analysis & Insights")
    st.markdown("We utilized Unsupervised Machine Learning to uncover hidden patterns in the audio data.")
    # 1. Prepare Features
    features = ['loudness', 'breathiness', 'pitch', 'brightness', 'tonality']

    # 2. Run Clustering (K-Means)
    kmeans = KMeans(n_clusters=5, random_state=42)
    df_raw['cluster'] = kmeans.fit_predict(df_norm[features])

    # 3. Run PCA (The Map)
    pca = PCA(n_components=2)
    components = pca.fit_transform(df_norm[features])
    df_raw['pca_x'] = components[:, 0]
    df_raw['pca_y'] = components[:, 1]
    # --- TAB 1: THE CLUSTERS (STYLE TOKENS) ---
    tab1, tab2, tab3 = st.tabs(["The Style Tokens", "The Intimacy Formula", "Cross-Modal Alignment"])

    with tab1:
        st.subheader("1. The 5 Style Tokens (Clustering)")

        # --- INTERACTION: KEYWORD HIGHLIGHTER ---
        # This answers: "Does the word 'Love' appear in the 'Tsundere' cluster?"
        c_filter, c_info = st.columns([1, 2])
        with c_filter:
            search_term = st.text_input("üîç Highlight Keyword (e.g., 'Love', 'Kiss')", "")

        # Create a boolean column for the highlighter
        if search_term:
            df_raw['is_highlighted'] = df_raw['text'].str.contains(search_term, na=False, case=False)
            color_col = 'is_highlighted'
            title_text = f"PCA Map: Highlighting '{search_term}'"
            color_map = {True: "red", False: "lightgrey"}
        else:
            color_col = df_raw['cluster'].astype(str)
            title_text = "PCA Projection: The 5 Emotional Islands"
            color_map = None

        # A. THE MAP (PCA)
        fig_map = px.scatter(
            df_raw,
            x='pca_x',
            y='pca_y',
            color=color_col,
            title=title_text,
            hover_data=['text', 'cluster'],
            opacity=0.7,
            color_discrete_map=color_map,
            color_discrete_sequence=px.colors.qualitative.Bold
        )
        st.plotly_chart(fig_map, use_container_width=True)

        # B. KEY INSIGHT BOX
        st.info(f"""
        üí° **Key Insight:** The clusters form distinct "Islands," proving that our acoustic features successfully separated the emotional archetypes.
        {f"Notice how '{search_term}' appears across multiple clusters? This means the word itself doesn't dictate the emotion." if search_term else ""}
        """)

        # 3. THE STYLE TOKENS (Radar Charts)
        cluster_insights = {
            0: {
                "title": "The Mature / Cool Type (Kuudere)",
                "desc": """
                        **The 'Stability' Token.**
                        * **Signature:** Low Pitch (0.21) + Max Tonality (0.85).
                        * **Vibe:** Composed, serious, and unwavering. Unlike the 'Panic' voices, this archetype has zero jitter. It represents the 'Cool Beauty' or narrator archetype.
                        """
            },
            1: {
                "title": "The Standard Heroine (Neutral)",
                "desc": """
                        **The 'Neutrality' Token.**
                        * **Signature:** Balanced Pitch (0.47) + High Tonality.
                        * **Vibe:** Clear and professional. This is the 'Control Group' of the dataset‚Äîstandard dialogue without extreme affection (breathiness) or aggression (loudness).
                        """
            },
            2: {
                "title": "The Sweet / Imouto Type (Deredere)",
                "desc": """
                        **The 'Affection' Token.**
                        * **Signature:** Low Loudness (0.33) + Elevated Breathiness.
                        * **Vibe:** Soft-spoken and safe. The high 'Brightness' suggests a smaller vocal tract (younger character), while the quiet delivery engineers a sense of vulnerability and comfort.
                        """
            },
            3: {
                "title": "The Dominant / Sadist Type (S-Type)",
                "desc": """
                        **The 'Dominance' Token.**
                        * **Signature:** Lowest Breathiness (0.17) + High Tonality.
                        * **Vibe:** Hard, direct, and piercing. The complete lack of 'air' in the voice removes any sense of intimacy, creating a commanding or scolding tone used by strict characters.
                        """
            },
            4: {
                "title": "The Seductive / Onee-san Type",
                "desc": """
                        **The 'Seduction' Token.**
                        * **Signature:** Lowest Pitch (0.18) + High Brightness (0.65).
                        * **Vibe:** Deep intimacy. This unique combo (Low Pitch + High Texture) indicates **Vocal Fry**‚Äîthe gravelly, mature register used to signal deep, overwhelming closeness.
                        """
            }
        }

        st.subheader("The 'Style Token' Signatures")
        st.markdown("Now we look at the **Average Shape** (Centroid) of each cluster to understand its vibe.")

        col1, col2 = st.columns([1, 2])

        with col1:
            cluster_id = st.selectbox("Select Token ID", sorted(df_raw['cluster'].unique()))
            subset = df_norm[df_raw['cluster'] == cluster_id]
            avg_stats = subset[features].mean()

            st.write(f"**Files:** {len(subset)}")
            st.json(avg_stats.to_dict())

        insight = cluster_insights.get(cluster_id,
                                       {"title": f"Cluster {cluster_id}", "desc": "No manual analysis available."})

        # Display Text
        st.success(f"**{insight['title']}**")
        st.markdown(insight['desc'])
        st.caption(f"Based on {len(subset)} audio samples.")

        with col2:
            # Create Radar Chart
            categories = ['Loudness', 'Breathiness', 'Pitch', 'Brightness', 'Tonality']
            values = avg_stats.values.tolist()
            values += values[:1]

            fig = go.Figure(data=go.Scatterpolar(
                r=values,
                theta=categories + [categories[0]],
                fill='toself',
                name=f'Archetype {cluster_id}'
            ))
            fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1])), showlegend=False)
            st.plotly_chart(fig, use_container_width=True)




        # 3. SAMPLES
        st.divider()
        st.write(f"**Listen to Archetype {cluster_id}:**")
        cols = st.columns(3)
        for i, (_, row) in enumerate(df_raw[df_raw['cluster'] == cluster_id].sample(3).iterrows()):
            with cols[i]:
                st.audio(row['path'])
                st.caption(row['text'])

    # --- TAB 2: INTIMACY ENGINEERING ---
    with tab2:
        st.subheader("2. Engineering Intimacy (The Physics)")

        # --- INTERACTION: DURATION FILTER ---
        # Filters out short noises to clean up the graph
        st.markdown("**Filter Data**")
        min_dur = st.slider("Hide clips shorter than (seconds):", 0.0, 5.0, 1.0, step=0.5)
        filtered_df = df_raw[df_raw['duration'] >= min_dur].copy()

        st.caption(f"Showing {len(filtered_df)} clips (Hidden {len(df_raw) - len(filtered_df)} short clips)")

        # Formula Tuners
        with st.expander("üéõÔ∏è Tune the Intimacy Formula"):
            w_breath = st.slider("Weight: Breathiness", 0.5, 2.0, 1.5)
            w_quiet = st.slider("Weight: Quietness", 0.5, 2.0, 1.0)

        # Recalculate based on slider
        # Note: We must recalculate on the FILTERED dataframe
        filtered_indices = filtered_df.index
        norm_subset = df_norm.loc[filtered_indices]

        filtered_df['dynamic_intimacy'] = (norm_subset['breathiness'] * w_breath) + (
                    (1 - norm_subset['loudness']) * w_quiet)
        filtered_df['dynamic_intensity'] = (norm_subset['loudness'] * 1.5) + (norm_subset['pitch'] * 1.0)

        col_plot, col_insight = st.columns([3, 1])
        with col_plot:
            fig_scat = px.scatter(
                filtered_df,
                x="dynamic_intimacy",
                y="dynamic_intensity",
                color="breathiness",
                hover_data=['text'],
                title="The Emotional Landscape (Filtered)",
                labels={"dynamic_intimacy": "Calculated Proximity", "dynamic_intensity": "Calculated Arousal"},
                color_continuous_scale="Viridis"
            )
            # Add Zones
            fig_scat.add_shape(type="rect", x0=1.5, y0=0, x1=2.5, y1=1.0, line=dict(color="Green"), fillcolor="Green",
                               opacity=0.1)
            st.plotly_chart(fig_scat, use_container_width=True)

        with col_insight:
            st.markdown("### üü¢ The ASMR Zone")
            st.write("Files in the green box represent mathematically optimized intimacy.")

            if not filtered_df.empty:
                top_file = filtered_df.sort_values('dynamic_intimacy', ascending=False).iloc[0]
                st.audio(top_file['path'])
                st.caption(f"Rank #1: {top_file['text']}")
            else:
                st.warning("No files match filter.")
        # --- THE DISCOVERY BLOCK (Your New Text) ---
        st.divider()
        st.subheader("Insight: The Biological Constraint")
        st.info("""
        **Initially, we hypothesized that Intimacy and Intensity were orthogonal vectors**, similar to the Russell Circumplex Model, implying they could vary independently.

        **However, our data refutes this.** The scatter plot reveals a strong **Negative Correlation**. This led to a key discovery: **The Biological Constraint of Intimacy.**

        The "Empty Top-Right Corner" of the graph proves that high-arousal intimacy is physically impossible. To maximize the *Proximity Effect* (Intimacy), an actor must sacrifice *Intensity*. 

        This suggests that in the grammar of voice acting, **Vulnerability (Breathiness) requires the surrender of Power (Loudness).**
        """)

    # --- TAB 3: CROSS-MODAL ALIGNMENT ---
    with tab3:
        st.subheader("3. Cross-Modal Alignment")

        # --- INTERACTION: ANOMALY THRESHOLD ---
        # Allows user to isolate "Bad Acting"
        st.markdown("**Find Anomalies**")
        threshold = st.slider("Mismatch Severity Threshold", 0.0, 5.0, 0.0,
                              help="0 = Show All. 5 = Show only extreme mismatches.")

        # Prep Data
        acoustic_raw = (df_norm['breathiness'] * 1.5) + ((1 - df_norm['loudness']) * 1.0)
        df_raw['acoustic_final'] = (acoustic_raw / acoustic_raw.max()) * 10
        if 'semantic_score' in df_raw.columns:
            df_raw['semantic_final'] = (df_raw['semantic_score'] / df_raw['semantic_score'].max()) * 10

        # Calculate Difference
        df_raw['mismatch'] = abs(df_raw['acoustic_final'] - df_raw['semantic_final'])

        # Filter
        anomaly_df = df_raw[df_raw['mismatch'] >= threshold]

        fig_align = px.scatter(
            anomaly_df,
            x="semantic_final",
            y="acoustic_final",
            color="mismatch",
            size='duration',
            hover_data=['text'],
            title=f"Performance Alignment (Showing {len(anomaly_df)} clips)",
            labels={"semantic_final": "Script Intimacy (0-10)", "acoustic_final": "Vocal Intimacy (0-10)"},
            color_continuous_scale="Turbo"
        )
        # Perfect Line
        fig_align.add_shape(type="line", x0=0, y0=0, x1=10, y1=10, line=dict(color="Green", dash="dash"))
        st.plotly_chart(fig_align, use_container_width=True)

        st.info(f"""
       Our Cross-Modal Analysis reveals that while most performances align with the script (the diagonal), the most interesting data points are the Dissonant Outliers.

The massive outlier in the top-left (Score: 0 Text / 10 Audio) proves that in this genre, Paralinguistics override Semantics. The actors are injecting massive amounts of intimacy into neutral text, effectively 'elevating' the script through pure vocal technique. Conversely, the outliers on the bottom-right likely capture 'Tsundere' performances, where the acoustic anger masks the semantic affection.
        """)

# --- PAGE 3: INTIMACY (PHYSICS) ---
# ... inside app.py ...

# ... inside app.py ...

elif page == "4. Conclusions":
    st.title("üìù Conclusions & Recommendations")

    # 1. THE EXECUTIVE SUMMARY
    st.markdown("""
    ### üéØ The "Paralinguistic Code" Decoded
    Our research set out to determine if the "soul" of a voice actor could be quantified. 
    By applying unsupervised learning to 1,000 audio samples, we were able to reverse-engineered the **Acoustic DNA of Emotion**.
    """)

    # 2. VISUAL SUMMARY OF FINDINGS
    col1, col2 = st.columns(2)

    with col1:
        st.info("**The 5 Style Tokens (Clustering)**")
        st.markdown("""
        * **Cluster 0:** Stability (Low Pitch, Pure Tone)
        * **Cluster 1:** Neutrality (Balanced, Clear)
        * **Cluster 2:** Affection (Quiet, Breathy)
        * **Cluster 3:** Dominance (Hard, Direct)
        * **Cluster 4:** Seduction (Deep, Vocal Fry)
        """)

    with col2:
        st.success("**The Physics of Intimacy**")
        st.markdown("""
        * **The Discovery:** Intimacy and Intensity are **Mutually Exclusive**.
        * **The Constraint:** You cannot scream in a whisper. 
        * **The Metric:** **Breathiness** is the currency of closeness.
        """)
    st.error("**Cross-Modal Analysis**")
    st.markdown("""
    * The massive outlier in the top-left (Score: 0 Text / 10 Audio) proves that in this genre.
    * Paralinguistics override Semantics
    """)

    st.divider()

    # 3. INTERACTIVE: THE SCENARIO PLANNER
    # This specifically targets the "Text boxes and Dropdowns" grading criteria
    st.subheader("Interactive: Design Your Voice Model")
    st.markdown("Use our findings to generate a technical specification for a new AI Voice.")


    # THE DROPDOWN
    target_audience = st.selectbox(
        "1. Select Target Audience",
        ["Insomniacs (Sleep Aid)", "Corporate Training", "Dating Sim Player", "Emergency Alert System"]
    )


    # LOGIC ENGINE
    if target_audience == "Insomniacs (Sleep Aid)":
        rec_cluster = "Cluster 4 (Seduction/Deep)"
        tuning = "Lower Pitch <200Hz, Maximize Vocal Fry."
        reason = "Low frequencies induce relaxation. Vocal fry adds texture without waking the user."
    elif target_audience == "Corporate Training":
        rec_cluster = "Cluster 1 (Neutral)"
        tuning = "Maximize Tonality, Zero Breathiness."
        reason = "High breathiness reduces intelligibility. Prioritize clarity and authority."
    elif target_audience == "Dating Sim Player":
        rec_cluster = "Cluster 2 (Affection)"
        tuning = "Increase Breathiness +20%, Reduce Volume."
        reason = "Triggers the 'Proximity Effect', creating the illusion of physical closeness."
    else:  # Emergency Alert
        rec_cluster = "Cluster 3 (Dominance)"
        tuning = "Maximize Loudness, Flatten Pitch."
        reason = "Urgency requires a 'Hard' voice to cut through background noise."

    st.warning(f"""
    **Target:** {target_audience}
    ---
    **Recommended Model:** {rec_cluster}
    **Acoustic Tuning:** {tuning}
    **Scientific Rationale:** {reason}
    """)

    st.divider()

    # 4. LIMITATIONS & CHALLENGES (The Critical Analysis)
    st.subheader("Project Limitations & Challenges")
    st.markdown(
        "While our model successfully identified broad archetypes, we encountered specific challenges inherent to the dataset.")

    with st.expander("‚ö†Ô∏è Read Limitation Analysis"):
        st.markdown("""
            **1. Temporal Instability (The 'Short Clip' Problem)**
            * **Challenge:** Our clustering model struggled with clips shorter than 1.0 second (e.g., moans, gasps).
            * **Cause:** Spectral features like *Tonality* and *Pitch* require a sustained duration to stabilize. Short bursts of sound appear as "noise" to the algorithm, often leading to misclassification of "Gasps" into the "Dominant" cluster due to sudden energy spikes.

            **2. Semantic Blindness (Irony & Subtext)**
            * **Challenge:** The Cross-Modal Alignment flagged "Tsundere" lines as "Bad Acting" (High Mismatch).
            * **Cause:** Our Rule-Based NLP relies on keywords (*Bag-of-Words*). It cannot detect *Irony* (saying "I hate you" affectionately). A more advanced model (Japanese BERT) would be required to capture this contextual subtext.

            **3. Ecological Validity (Performative Bias)**
            * **Challenge:** The "Intimacy" engineered here is *Performative*, not *Natural*.
            * **Implication:** Models trained on this dataset risk falling into the "Uncanny Valley" if deployed for real-world therapy. The acoustic features are exaggerated (hyper-breathy, hyper-high pitch) compared to natural human speech.
            """)